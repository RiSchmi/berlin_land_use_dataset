{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different weighting schemes for emission average at unique time-steps to determine spatial correlation characteristics  \n",
    "\n",
    "- city-center distance weight: monitoring sites with a similar distance to the city center are weighted higher\n",
    "- inverse distance weighting (IDW): near-by locations are weighted higher \n",
    "- no spatial weighting: average of all stations at same time step   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "print(gpd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_step</th>\n",
       "      <th>id</th>\n",
       "      <th>NO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023010100</td>\n",
       "      <td>mc010</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023010101</td>\n",
       "      <td>mc010</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023010102</td>\n",
       "      <td>mc010</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023010103</td>\n",
       "      <td>mc010</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023010104</td>\n",
       "      <td>mc010</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_step     id   NO2\n",
       "0  2023010100  mc010   7.0\n",
       "1  2023010101  mc010  19.0\n",
       "2  2023010102  mc010   9.0\n",
       "3  2023010103  mc010   5.0\n",
       "4  2023010104  mc010   6.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# read sites and adjust naming\n",
    "sites = gpd.read_file('../data/monitoring_station/monitoring_station.shp')[['id', 'geometry']]\n",
    "sites['id'] = sites['id'].apply(lambda x : x.lower().replace(' ', '')[:5])\n",
    "sites = sites[(sites['id']!='mc014')&(sites['id']!='mc085')]\n",
    "\n",
    "# read dataframe with target variable and predictor features for all timesteps\n",
    "main = pd.read_csv('../datasets/df_pollution_2023_berlin_imputed.csv')[['time_step', 'id',\t'NO2']]\n",
    "main.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial similarity estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation for city center weighted average: 0.717581091107383\n",
      "Mutual Information for city center weighted average: [0.35371728].\n"
     ]
    }
   ],
   "source": [
    "# city-center distance weight (CCDW)\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. add distance to Fehrnsehturm as distance to berlin center\n",
    "\n",
    "def return_distance_center(point, latitude = 52.520803, longitude = 13.40945):\n",
    "    '''\n",
    "    return distance in meter between point (geometry) and \n",
    "    second point specified by lat and long (EPSG:4326)\n",
    "    '''\n",
    "    center_point = gpd.GeoDataFrame(index=[0], crs='EPSG:4326', geometry=[Point(longitude, latitude)])\n",
    "    center_point = center_point.to_crs('EPSG:25833')\n",
    "    center_geometry = center_point.geometry[0]\n",
    "    return center_geometry.distance(point)\n",
    "\n",
    "sites['distance_city'] = sites['geometry'].apply(lambda x: return_distance_center(point =x)/1000) # in meter \n",
    "\n",
    "#merge with previous file\n",
    "main = pd.merge(main, sites, on = 'id', how= 'outer').drop(['geometry'], axis= 1)\n",
    "\n",
    "# 2. calculate weighted mean pollution based on similarity in distance to the city center\n",
    "\n",
    "def cc_weighted_mean(row):\n",
    "    # Filter for other stations at the same timestamp\n",
    "    same_time = main[main['MESS_DATUM'] == row['MESS_DATUM']]\n",
    "    # Exclude the current station's measurement\n",
    "    other_stations = same_time[same_time['id'] != row['id']]\n",
    "    \n",
    "    if other_stations.empty:\n",
    "        return np.nan  # Return NaN if no other stations are available at the same time\n",
    "    \n",
    "    # Calculate similarity weights based on the inverse of the absolute difference in distances\n",
    "    weights = 1 / (1 + np.abs(other_stations['distance_city'] - row['distance_city']))\n",
    "       \n",
    "    # Calculate the weighted mean pollution\n",
    "    weighted_mean = np.average(other_stations['NO2'], weights=weights)\n",
    "    return weighted_mean\n",
    "\n",
    "# Apply the function to each row to calculate the new feature\n",
    "main['cc_weighted_mean_pollution'] = main.apply(cc_weighted_mean, axis=1)\n",
    "\n",
    "# 3. return correlate\n",
    "correlation, p_value = main['NO2'].corr(main['cc_weighted_mean_pollution'], method='pearson'), None\n",
    "print(f\"Pearson correlation for city center weighted average: {correlation}\")\n",
    "\n",
    "# 4. return mutual information\n",
    "mi_score = mutual_info_regression(main.filter(['NO2']), main['cc_weighted_mean_pollution'])\n",
    "print(f\"Mutual Information for city center weighted average: {mi_score}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation for sqrt city center weighted average: 0.6654973088619369\n",
      "Mutual Information for sqrt city center weighted average: [0.27772331].\n"
     ]
    }
   ],
   "source": [
    "# city-center (sqrt difference) distance weight (CCDW)\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "# 1. calculate weighted mean pollution based on similarity in distance to the city center\n",
    "\n",
    "def cc_weighted_mean(row):\n",
    "    # Filter for other stations at the same timestamp\n",
    "    same_time = main[main['MESS_DATUM'] == row['MESS_DATUM']]\n",
    "    # Exclude the current station's measurement\n",
    "    other_stations = same_time[same_time['id'] != row['id']]\n",
    "    \n",
    "    if other_stations.empty:\n",
    "        return np.nan  # Return NaN if no other stations are available at the same time\n",
    "    \n",
    "    # Calculate similarity weights based on the inverse of the absolute difference in distances\n",
    "    weights = 1 / ((np.abs(other_stations['distance_city'] - row['distance_city']))**(1/4))\n",
    "    \n",
    "    # Calculate the weighted mean pollution\n",
    "    weighted_mean = np.average(other_stations['NO2'], weights=weights)\n",
    "    return weighted_mean\n",
    "\n",
    "# Apply the function to each row to calculate the new feature\n",
    "main['cc_sqrt_weighted_mean_pollution'] = main.apply(cc_weighted_mean, axis=1)\n",
    "\n",
    "# 3. return correlate\n",
    "correlation, p_value = main['NO2'].corr(main['cc_sqrt_weighted_mean_pollution'], method='pearson'), None\n",
    "print(f\"Pearson correlation for sqrt city center weighted average: {correlation}\")\n",
    "\n",
    "# 4. return mutual information\n",
    "mi_score = mutual_info_regression(main.filter(['NO2']), main['cc_sqrt_weighted_mean_pollution'])\n",
    "print(f\"Mutual Information for sqrt city center weighted average: {mi_score}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation for inverse distance average: 0.648084059409781\n",
      "Mutual Information for inverse distance average: [0.25721471].\n"
     ]
    }
   ],
   "source": [
    "# inverse distance weighting (IDW)\n",
    "import numpy as np\n",
    "# 1. calculate distance between all station\n",
    "def calculate_distance_matrix(sites):\n",
    "    \"\"\"Calculate the pairwise distance matrix for sites.\"\"\"\n",
    "\n",
    "    site_points = sites['geometry']\n",
    "    num_sites = len(site_points)\n",
    "    dist_matrix = np.zeros((num_sites, num_sites))\n",
    "\n",
    "    for i in range(num_sites):\n",
    "        for j in range(num_sites):\n",
    "            # Calculate distance between points\n",
    "            dist_matrix[i, j] = site_points[i].distance(site_points[j]) / 1000 # convert to km\n",
    "\n",
    "    dist_matrix = pd.DataFrame(dist_matrix, index= sites['id'], columns= sites['id'])\n",
    "    return dist_matrix\n",
    "\n",
    "dist_matrix = calculate_distance_matrix(sites= sites.reset_index())\n",
    "\n",
    "# 2. calculate inverse distance weighted average of pollution\n",
    "\n",
    "def inverse_distance_weighting(row, main, dist_matrix):\n",
    "    # filter for correct date\n",
    "    same_time = main[main['MESS_DATUM'] == row['MESS_DATUM']]\n",
    "    # Exclude the current station's measurement\n",
    "    other_stations = same_time[same_time['id'] != row['id']].sort_values('id')\n",
    "\n",
    "    # Get distances and sort to ensure order matches\n",
    "    distances = dist_matrix.loc[row['id'], other_stations['id']]\n",
    "    \n",
    "    # inverse distance + normalization    \n",
    "    inverse_distances = 1 / distances\n",
    "    inverse_distances = inverse_distances/ inverse_distances.sum()\n",
    "    \n",
    "    # Calculate the weighted mean pollution, ensuring the order matches\n",
    "    weighted_mean = np.average(\n",
    "        other_stations['NO2'],\n",
    "        weights=inverse_distances\n",
    "    )\n",
    "    \n",
    "    return weighted_mean\n",
    "\n",
    "# Calculate the inverse distance mean\n",
    "main['inverse_distance_pollution'] = main.apply(\n",
    "    lambda row: inverse_distance_weighting(row, main, dist_matrix),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 3. return correlation\n",
    "correlation = main['NO2'].corr(main['inverse_distance_pollution'], method='pearson')\n",
    "print(f\"Pearson correlation for inverse distance average: {correlation}\")\n",
    "\n",
    "# 4. return mutual information\n",
    "mi_score = mutual_info_regression(main.filter(['NO2']), main['inverse_distance_pollution'])\n",
    "print(f\"Mutual Information for inverse distance average: {mi_score}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation for non-weighted average in pollution: 0.623769773853018\n",
      "Mutual Information for non- weighted average: [0.29397268].\n"
     ]
    }
   ],
   "source": [
    "# non- weighted average\n",
    "\n",
    "def non_weighted_average(row, main):\n",
    "    # filter for correct date\n",
    "    same_time = main[main['MESS_DATUM'] == row['MESS_DATUM']]\n",
    "    # Exclude the current station's measurement\n",
    "    other_stations = same_time[same_time['id'] != row['id']].sort_values('id')\n",
    "\n",
    "    weighted_mean = np.average(other_stations['NO2'])\n",
    "    \n",
    "    return weighted_mean\n",
    "\n",
    "# Calculate the inverse distance mean\n",
    "main['non_weighted_mean_pollution'] = main.apply(lambda row: non_weighted_average(row, main), axis=1)\n",
    "\n",
    "# Correlation\n",
    "correlation = main['NO2'].corr(main['non_weighted_mean_pollution'], method='pearson')\n",
    "print(f\"Pearson correlation for non-weighted average in pollution: {correlation}\")\n",
    "\n",
    "# 4. return mutual information\n",
    "mi_score = mutual_info_regression(main.filter(['NO2']), main['non_weighted_mean_pollution'])\n",
    "print(f\"Mutual Information for non- weighted average: {mi_score}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results p. 15 of the Thesis:\n",
    "\n",
    "\"For empirical evaluation, a Pearson correlation test and a mutual information (MI) estimation are conducted to compare the true emission values with the weighted averages calculated by each method. The inverse distance method achieves a correlation of 0.648, surpassing the unweighted average. The proposed difference in distance approach achieves the best correlation score of 0.718. The entropy-based MI exceeds covariance (Kraskov et al., 2004, p. 1) and demonstrates the increased informative value of the difference in distance calculation,\n",
    "with a 40% increase compared to the inverse distance. Nevertheless, the simplified distance-based similarity assumption and small sample of stations limit this evaluation and the different spatial proximity estimations are further compared through model performance.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
